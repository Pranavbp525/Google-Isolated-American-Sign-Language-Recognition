{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport pyarrow.parquet as pq\nfrom tqdm import tqdm\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ndata=[]\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if filename.endswith('.parquet'):\n            data.append(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-27T01:27:54.278981Z","iopub.execute_input":"2023-04-27T01:27:54.279468Z","iopub.status.idle":"2023-04-27T01:28:43.670926Z","shell.execute_reply.started":"2023-04-27T01:27:54.279426Z","shell.execute_reply":"2023-04-27T01:28:43.669738Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.decomposition import PCA, IncrementalPCA\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nimport warnings\n\n\n#Finding the ideal value for n_components that fits all the parquet files with 90% variance retained\n\ndef expectedVarianceRatioCumSum(data):\n    preprocessing_pipeline = make_pipeline(StandardScaler(), SimpleImputer(strategy= 'constant',fill_value = 0))\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        scaled_data = preprocessing_pipeline.fit_transform(data)\n    pca = PCA(n_components = min(scaled_data.shape[0],scaled_data.shape[1]))\n    pca.fit(scaled_data)\n    return pca.explained_variance_ratio_.cumsum()\n    \n\ndef find_best_n_components():\n    variances = []\n    for path in tqdm(data):\n        df = pd.read_parquet(path)\n        pivoted = preprocess_data(df)\n        variances.append(expectedVarianceRatioCumSum(pivoted))\n    return variances\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T18:44:05.516750Z","iopub.execute_input":"2023-04-27T18:44:05.517204Z","iopub.status.idle":"2023-04-27T18:44:05.527421Z","shell.execute_reply.started":"2023-04-27T18:44:05.517166Z","shell.execute_reply":"2023-04-27T18:44:05.526434Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"variances = find_best_n_components()\nlen(variances)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T01:32:48.083221Z","iopub.execute_input":"2023-04-27T01:32:48.083741Z","iopub.status.idle":"2023-04-27T03:31:07.918667Z","shell.execute_reply.started":"2023-04-27T01:32:48.083698Z","shell.execute_reply":"2023-04-27T03:31:07.914872Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 94477/94477 [1:58:19<00:00, 13.31it/s]  \n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"94477"},"metadata":{}}]},{"cell_type":"code","source":"components = []\nfor variance in variances:\n    for i in range(len(variance)):\n        if variance[i]>0.9:\n            components.append(i+1)\n            break","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:31:43.081826Z","iopub.execute_input":"2023-04-27T03:31:43.083594Z","iopub.status.idle":"2023-04-27T03:31:43.329326Z","shell.execute_reply.started":"2023-04-27T03:31:43.083526Z","shell.execute_reply":"2023-04-27T03:31:43.327903Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"len(components)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:31:47.639447Z","iopub.execute_input":"2023-04-27T03:31:47.640573Z","iopub.status.idle":"2023-04-27T03:31:47.649950Z","shell.execute_reply.started":"2023-04-27T03:31:47.640519Z","shell.execute_reply":"2023-04-27T03:31:47.648266Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"94477"},"metadata":{}}]},{"cell_type":"code","source":"import statistics\nmode = statistics.mode(components)\nmaxi = max(components)\nprint(f'Maximum n-component for entire dataset: {maxi}\\n Most frequent n-component for entire dataset:{mode}')","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:31:54.171246Z","iopub.execute_input":"2023-04-27T03:31:54.172632Z","iopub.status.idle":"2023-04-27T03:31:54.188792Z","shell.execute_reply.started":"2023-04-27T03:31:54.172580Z","shell.execute_reply":"2023-04-27T03:31:54.187353Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Maximum n-component for entire dataset: 13\n Most frequent n-component for entire dataset:5\n","output_type":"stream"}]},{"cell_type":"code","source":"def padZeros(data):\n    rows_to_add = maxi - len(data)\n    padded_data = {}\n    for column in data.columns:\n        padded_data[column] = [0] * rows_to_add\n    return data.append(pd.DataFrame(padded_data))\n\ndef scaleData(data):\n    preprocessing_pipeline = make_pipeline(StandardScaler(), SimpleImputer(strategy= 'constant',fill_value = 0))\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        scaled_data = pd.DataFrame(preprocessing_pipeline.fit_transform(data))\n    return scaled_data\n\ndef preprocess_data(data):\n    #dropping most of the face landmarks\n    data = data.drop([\"z\"],axis=1)\n    keep_indices = [0, 9, 11, 13, 14, 17, 117, 118, 119, 199, 346, 347, 348]\n    mask = (data['type'] == 'face') & (~data['landmark_index'].isin(keep_indices))\n    data.drop(data[mask].index,inplace=True)\n    #dropping unnecessary columns\n    data = data.drop([\"row_id\",\"type\",\"landmark_index\"], axis=1)\n    #pivoting data such that each frame can be represented as each row\n    data['index'] = data.groupby('frame').cumcount()\n    pivoted = data.pivot(index='frame', columns='index',values=['x','y'])\n    new_cols=[name[0]+str(name[1]) for name in pivoted.columns]\n    pivoted.columns=new_cols\n    data = data.drop(['index'],axis=1)\n    return pivoted\n    \n\ndef applyPCA(data):\n    data = preprocess_data(data)\n    preprocessing_pipeline = make_pipeline(StandardScaler(), SimpleImputer(strategy= 'constant',fill_value = 0))\n    scaled_data = scaleData(data)\n    if len(scaled_data<maxi):\n        scaled_data = padZeros(scaled_data)\n    pca = PCA(n_components = maxi)\n    pca.fit(scaled_data)\n    pca_data = pca.transform(scaled_data)\n    return pd.DataFrame(pca_data)\n    \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:31:57.921723Z","iopub.execute_input":"2023-04-27T03:31:57.922239Z","iopub.status.idle":"2023-04-27T03:31:57.938661Z","shell.execute_reply.started":"2023-04-27T03:31:57.922198Z","shell.execute_reply":"2023-04-27T03:31:57.937164Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"input_dir = '/kaggle/input/asl-signs/train_landmark_files'\noutput_dir = 'pca_files/train_landmark_files'\nfor directory in tqdm(data):\n    # extract the common part of the file path\n    path_parts = directory.split(\"/\")\n    frame_num = path_parts[-2]\n    file_name = path_parts[-1]\n    input_file_path = os.path.join(input_dir, frame_num, file_name)\n    output_file_path = os.path.join(output_dir, frame_num, file_name)\n    \n    # read the parquet file into a pandas dataframe\n    df = pd.read_parquet(directory)\n    \n    # preprocess the data\n    new_data = applyPCA(df)\n    \n    new_cols=[\"z\"+str(name) for name in new_data.columns]\n    new_data.columns = new_cols\n    \n    # save the data to a new directory\n    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n    new_data.to_parquet(output_file_path)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-04-27T03:32:02.378523Z","iopub.execute_input":"2023-04-27T03:32:02.379045Z","iopub.status.idle":"2023-04-27T06:04:06.469502Z","shell.execute_reply.started":"2023-04-27T03:32:02.378998Z","shell.execute_reply":"2023-04-27T06:04:06.463883Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"100%|██████████| 94477/94477 [2:32:04<00:00, 10.35it/s]  \n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nfrom IPython.display import FileLink\n\n# Navigate to the directory where the output folder is located\nos.chdir('/kaggle/working/')\n\n# Zip the output folder\n!zip -r pca_files1.zip pca_files\n\n# Create a download link for the zipped folder\nFileLink(r'pca_files1.zip')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pca_files[948]","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:49:43.534612Z","iopub.execute_input":"2023-04-27T07:49:43.535090Z","iopub.status.idle":"2023-04-27T07:49:43.543310Z","shell.execute_reply.started":"2023-04-27T07:49:43.535051Z","shell.execute_reply":"2023-04-27T07:49:43.541965Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/pca_files/train_landmark_files/32319/664221861.parquet'"},"metadata":{}}]},{"cell_type":"code","source":"pca_files = []\nfor dirname, _, filenames in os.walk('/kaggle/input/pca-data'):\n    for filename in filenames:\n        if filename.endswith('.parquet'):\n            pca_files.append(os.path.join(dirname, filename))\nlen(pca_files)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:22:32.481606Z","iopub.execute_input":"2023-04-27T20:22:32.482887Z","iopub.status.idle":"2023-04-27T20:23:09.395159Z","shell.execute_reply.started":"2023-04-27T20:22:32.482844Z","shell.execute_reply":"2023-04-27T20:23:09.394003Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"94477"},"metadata":{}}]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/asl-signs/train.csv\")\ntrain","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:23:09.399359Z","iopub.execute_input":"2023-04-27T20:23:09.399709Z","iopub.status.idle":"2023-04-27T20:23:09.645151Z","shell.execute_reply.started":"2023-04-27T20:23:09.399675Z","shell.execute_reply":"2023-04-27T20:23:09.644135Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                path  participant_id  \\\n0      train_landmark_files/26734/1000035562.parquet           26734   \n1      train_landmark_files/28656/1000106739.parquet           28656   \n2       train_landmark_files/16069/100015657.parquet           16069   \n3      train_landmark_files/25571/1000210073.parquet           25571   \n4      train_landmark_files/62590/1000240708.parquet           62590   \n...                                              ...             ...   \n94472   train_landmark_files/53618/999786174.parquet           53618   \n94473   train_landmark_files/26734/999799849.parquet           26734   \n94474   train_landmark_files/25571/999833418.parquet           25571   \n94475   train_landmark_files/29302/999895257.parquet           29302   \n94476   train_landmark_files/36257/999962374.parquet           36257   \n\n       sequence_id    sign  \n0       1000035562    blow  \n1       1000106739    wait  \n2        100015657   cloud  \n3       1000210073    bird  \n4       1000240708    owie  \n...            ...     ...  \n94472    999786174   white  \n94473    999799849    have  \n94474    999833418  flower  \n94475    999895257    room  \n94476    999962374   happy  \n\n[94477 rows x 4 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>participant_id</th>\n      <th>sequence_id</th>\n      <th>sign</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_landmark_files/26734/1000035562.parquet</td>\n      <td>26734</td>\n      <td>1000035562</td>\n      <td>blow</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_landmark_files/28656/1000106739.parquet</td>\n      <td>28656</td>\n      <td>1000106739</td>\n      <td>wait</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_landmark_files/16069/100015657.parquet</td>\n      <td>16069</td>\n      <td>100015657</td>\n      <td>cloud</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_landmark_files/25571/1000210073.parquet</td>\n      <td>25571</td>\n      <td>1000210073</td>\n      <td>bird</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_landmark_files/62590/1000240708.parquet</td>\n      <td>62590</td>\n      <td>1000240708</td>\n      <td>owie</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>94472</th>\n      <td>train_landmark_files/53618/999786174.parquet</td>\n      <td>53618</td>\n      <td>999786174</td>\n      <td>white</td>\n    </tr>\n    <tr>\n      <th>94473</th>\n      <td>train_landmark_files/26734/999799849.parquet</td>\n      <td>26734</td>\n      <td>999799849</td>\n      <td>have</td>\n    </tr>\n    <tr>\n      <th>94474</th>\n      <td>train_landmark_files/25571/999833418.parquet</td>\n      <td>25571</td>\n      <td>999833418</td>\n      <td>flower</td>\n    </tr>\n    <tr>\n      <th>94475</th>\n      <td>train_landmark_files/29302/999895257.parquet</td>\n      <td>29302</td>\n      <td>999895257</td>\n      <td>room</td>\n    </tr>\n    <tr>\n      <th>94476</th>\n      <td>train_landmark_files/36257/999962374.parquet</td>\n      <td>36257</td>\n      <td>999962374</td>\n      <td>happy</td>\n    </tr>\n  </tbody>\n</table>\n<p>94477 rows × 4 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import json\ndef read_json(path):\n    with open(path, \"r\") as file:\n        json_data = json.load(file)\n    return json_data\ns2p_map = read_json(os.path.join(\"/kaggle/input/asl-signs/sign_to_prediction_index_map.json\"))\np2s_map = {v: k for k, v in s2p_map.items()}\n\nencoder = lambda x: s2p_map.get(x)\ndecoder = lambda x: p2s_map.get(x)\n\ntrain[\"label\"] =train[\"sign\"].map(encoder)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:23:09.646357Z","iopub.execute_input":"2023-04-27T20:23:09.647104Z","iopub.status.idle":"2023-04-27T20:23:09.705462Z","shell.execute_reply.started":"2023-04-27T20:23:09.647070Z","shell.execute_reply":"2023-04-27T20:23:09.704549Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train = train.drop([\"participant_id\",\"sequence_id\",\"sign\"],axis=1)\ntrain","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:23:09.707535Z","iopub.execute_input":"2023-04-27T20:23:09.707843Z","iopub.status.idle":"2023-04-27T20:23:09.724682Z","shell.execute_reply.started":"2023-04-27T20:23:09.707812Z","shell.execute_reply":"2023-04-27T20:23:09.723835Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                                                path  label\n0      train_landmark_files/26734/1000035562.parquet     25\n1      train_landmark_files/28656/1000106739.parquet    232\n2       train_landmark_files/16069/100015657.parquet     48\n3      train_landmark_files/25571/1000210073.parquet     23\n4      train_landmark_files/62590/1000240708.parquet    164\n...                                              ...    ...\n94472   train_landmark_files/53618/999786174.parquet    238\n94473   train_landmark_files/26734/999799849.parquet    108\n94474   train_landmark_files/25571/999833418.parquet     86\n94475   train_landmark_files/29302/999895257.parquet    188\n94476   train_landmark_files/36257/999962374.parquet    105\n\n[94477 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>path</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_landmark_files/26734/1000035562.parquet</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_landmark_files/28656/1000106739.parquet</td>\n      <td>232</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_landmark_files/16069/100015657.parquet</td>\n      <td>48</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_landmark_files/25571/1000210073.parquet</td>\n      <td>23</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_landmark_files/62590/1000240708.parquet</td>\n      <td>164</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>94472</th>\n      <td>train_landmark_files/53618/999786174.parquet</td>\n      <td>238</td>\n    </tr>\n    <tr>\n      <th>94473</th>\n      <td>train_landmark_files/26734/999799849.parquet</td>\n      <td>108</td>\n    </tr>\n    <tr>\n      <th>94474</th>\n      <td>train_landmark_files/25571/999833418.parquet</td>\n      <td>86</td>\n    </tr>\n    <tr>\n      <th>94475</th>\n      <td>train_landmark_files/29302/999895257.parquet</td>\n      <td>188</td>\n    </tr>\n    <tr>\n      <th>94476</th>\n      <td>train_landmark_files/36257/999962374.parquet</td>\n      <td>105</td>\n    </tr>\n  </tbody>\n</table>\n<p>94477 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def flattenData(path):\n    df = pd.read_parquet(path)\n    df = pad_or_truncate(df)\n    flatten = df.to_numpy().flatten()\n    return flatten\n\ndef pad_or_truncate(data):\n    if len(data)>50:\n        return truncate_start(data)\n    elif len(data)<50:\n        return pad_end(data)\n    else:\n        return data\ndef pad_end(data):\n    rows_to_add = 50 - len(data)\n    padded_data = {}\n    for column in data.columns:\n        padded_data[column] = [0] * rows_to_add\n    return data.append(pd.DataFrame(padded_data))\n\ndef truncate_start(data):\n    return data.iloc[:50, :]\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:23:13.488670Z","iopub.execute_input":"2023-04-27T20:23:13.489078Z","iopub.status.idle":"2023-04-27T20:23:13.497634Z","shell.execute_reply.started":"2023-04-27T20:23:13.489043Z","shell.execute_reply":"2023-04-27T20:23:13.496549Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"z.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-27T07:53:53.145664Z","iopub.execute_input":"2023-04-27T07:53:53.146836Z","iopub.status.idle":"2023-04-27T07:53:53.155218Z","shell.execute_reply.started":"2023-04-27T07:53:53.146785Z","shell.execute_reply":"2023-04-27T07:53:53.153759Z"},"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"(5, 650)"},"metadata":{}}]},{"cell_type":"code","source":"\ny = train['label'].values\nX = np.stack([flattenData(\"/kaggle/input/pca-data/pca_files/\"+i) for i in tqdm(train['path'])])","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:23:17.606445Z","iopub.execute_input":"2023-04-27T20:23:17.606857Z","iopub.status.idle":"2023-04-27T20:34:55.724983Z","shell.execute_reply.started":"2023-04-27T20:23:17.606819Z","shell.execute_reply":"2023-04-27T20:34:55.723850Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 94477/94477 [11:37<00:00, 135.41it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-27T20:35:24.995304Z","iopub.execute_input":"2023-04-27T20:35:24.995752Z","iopub.status.idle":"2023-04-27T20:35:25.002175Z","shell.execute_reply.started":"2023-04-27T20:35:24.995702Z","shell.execute_reply":"2023-04-27T20:35:25.000978Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(94477, 650)\n(94477,)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n\nclf = SVC(kernel='rbf', decision_function_shape='ovr')\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-27T21:52:24.223266Z","iopub.execute_input":"2023-04-27T21:52:24.223685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, classification_report\n\naccuracy = accuracy_score(y_test, y_pred)\nprint('Accuracy:', accuracy)\n\n# Compute the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion Matrix:\\n', cm)\n\n# Compute the classification report\ncr = classification_report(y_test, y_pred)\nprint('Classification Report:\\n', cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [v for k,v in p2s_map.items()],\n                     columns = [v for k,v in p2s_map.items()])\nplt.figure(figsize = (250,250))\nsn.heatmap(df_cm, annot=True)\nplt.savefig('SVM-CM.png')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}